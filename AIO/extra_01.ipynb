{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOswt2eq/6rQesLRSlxNBNi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-dHXfGErkWP","executionInfo":{"status":"ok","timestamp":1717780271508,"user_tz":-180,"elapsed":6,"user":{"displayName":"Đỗ Phúc Hảo","userId":"07577019584176057869"}},"outputId":"eb62991f-cf91-4269-86f8-a0385df98be2"},"outputs":[{"output_type":"stream","name":"stdout","text":["hello\n"]}],"source":["#Implementing the Self-Attention Mechanism from Scratch in PyTorch!\n"]},{"cell_type":"markdown","source":["```python\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define the Attention module\n","class Attention(nn.Module):\n","    \"\"\"\n","    This class implements a self-attention mechanism, which allows the model to\n","    attend to different parts of the input sequence based on their relevance.\n","    \"\"\"\n","    def __init__(self, d_in, d_out):\n","        \"\"\"\n","        Initializes the Attention module.\n","\n","        Args:\n","            d_in: Dimensionality of the input tensor (number of features).\n","            d_out: Dimensionality of the output tensor (number of attention heads).\n","        \"\"\"\n","        super().__init__()\n","        self.d_in = d_in\n","        self.d_out = d_out\n","\n","        # Define linear transformations for Keys, Queries, and Values\n","        # These transformations project the input tensor into different feature spaces.\n","        self.Q = nn.Linear(d_in, d_out)  # Query projection\n","        self.K = nn.Linear(d_in, d_out)  # Key projection\n","        self.V = nn.Linear(d_in, d_out)  # Value projection\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Performs the forward pass of the attention module.\n","\n","        Args:\n","            x: Input tensor of shape (batch_size, seq_len, d_in).\n","\n","        Returns:\n","            Output tensor of shape (batch_size, seq_len, d_out), representing the\n","            weighted average of the values based on the attention weights.\n","        \"\"\"\n","        # Project the input tensor into Keys, Queries, and Values\n","        queries = self.Q(x)  # (batch_size, seq_len, d_out)\n","        keys = self.K(x)    # (batch_size, seq_len, d_out)\n","        values = self.V(x)  # (batch_size, seq_len, d_out)\n","\n","        # Calculate the interaction matrix between Keys and Queries\n","        scores = torch.bmm(queries, keys.transpose(1, 2)) # (batch_size, seq_len, seq_len)\n","        scores = scores / (self.d_out ** 0.5)  # Scale scores for numerical stability\n","\n","        # Apply softmax to the scores to obtain attention weights\n","        attention = F.softmax(scores, dim=2)  # (batch_size, seq_len, seq_len)\n","\n","        # Compute the weighted average of the Values based on the attention weights\n","        hidden_states = torch.bmm(attention, values)  # (batch_size, seq_len, d_out)\n","\n","        return hidden_states\n","\n","# Example usage with MNIST dataset\n","if __name__ == \"__main__\":\n","    # Assuming you have the MNIST dataset loaded (e.g., using torchvision)\n","    # ...\n","    # Replace with your MNIST dataset loading code\n","    # ...\n","\n","    # Create an instance of the Attention module\n","    attention_layer = Attention(d_in=28*28, d_out=128)  # Example: 28x28 image size\n","\n","    # Example usage with a batch of MNIST images\n","    # images: Tensor of shape (batch_size, 1, 28, 28)\n","    # Flatten the images for the Attention module\n","    flattened_images = images.view(batch_size, -1)  # (batch_size, 28*28)\n","\n","    # Calculate the attention-weighted hidden states\n","    attention_output = attention_layer(flattened_images)\n","    # ...\n","    # Use the attention_output for further processing or classification\n","    # ...\n","```\n","\n","**Explanation:**\n","\n","**1. Initialization:**\n","   - The `__init__` method sets up the linear transformations for Keys, Queries, and Values. The dimensionality of the input (`d_in`) and output (`d_out`) are specified.\n","\n","**2. Forward Pass:**\n","   - `forward(x)` performs the core attention computation:\n","     - **Projection:** The input tensor `x` is projected into the Key, Query, and Value spaces using the linear transformations (`self.Q`, `self.K`, `self.V`).\n","     - **Interaction:** The dot product of Queries and Keys (after scaling) is computed to generate an interaction matrix called `scores`. This matrix represents the relevance of each input element to other elements in the sequence.\n","     - **Softmax:** Softmax is applied to the `scores` to obtain attention weights, which sum up to 1 for each input element. These weights indicate how much attention is given to each element in the sequence.\n","     - **Weighted Average:** The attention weights are used to compute a weighted average of the Values, resulting in a new representation called `hidden_states`. This representation captures the context-aware information from the input sequence.\n","\n","**3. MNIST Example:**\n","   - The example demonstrates how to use the `Attention` layer with the MNIST dataset.\n","   - The image is first flattened into a vector.\n","   - The `attention_layer` is applied to the flattened image to obtain the attention-weighted hidden states.\n","   - You can use these hidden states for further processing or classification tasks.\n","\n","**Key Points:**\n","\n","- The `Attention` module is a flexible building block that can be integrated into various deep learning models.\n","- It helps the model to focus on relevant parts of the input sequence, leading to improved performance in tasks like machine translation, text summarization, and image captioning.\n","- The scaling factor (`self.d_out ** 0.5`) helps to stabilize numerical computation by preventing large values from dominating the softmax calculation.\n","- In the MNIST example, the attention mechanism is applied to the flattened image, but it can be adapted to work with different input formats, such as sequences of words or time series data.\n","\n","This code provides a basic implementation of self-attention. More sophisticated attention mechanisms, such as multi-head attention and transformer architectures, build upon these fundamental concepts.\n"],"metadata":{"id":"59-TOr2jsYFd"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# Define the Attention module\n","class Attention(nn.Module):\n","    def __init__(self, d_in, d_out):\n","        super().__init__()\n","        self.d_in = d_in\n","        self.d_out = d_out\n","\n","        self.Q = nn.Linear(d_in, d_out)\n","        self.K = nn.Linear(d_in, d_out)\n","        self.V = nn.Linear(d_in, d_out)\n","\n","    def forward(self, x):\n","        queries = self.Q(x)\n","        keys = self.K(x)\n","        values = self.V(x)\n","\n","        scores = torch.bmm(queries, keys.transpose(1, 2))\n","        scores = scores / (self.d_out ** 0.5)\n","\n","        attention = F.softmax(scores, dim=2)\n","        hidden_states = torch.bmm(attention, values)\n","\n","        return hidden_states\n","\n","# Define the MNIST classifier model\n","class MNISTClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.attention = Attention(d_in=64*7*7, d_out=128)\n","        self.fc1 = nn.Linear(128, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = x.view(x.size(0), -1)  # Flatten the feature map\n","        x = x.view(x.size(0), 1, -1)  # Reshape for attention\n","        x = self.attention(x)\n","        x = x.view(x.size(0), -1)  # Flatten after attention\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Define the training parameters\n","batch_size = 64\n","epochs = 10\n","learning_rate = 0.001\n","\n","# Load the MNIST dataset\n","train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Initialize the model, loss function, and optimizer\n","model = MNISTClassifier()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","for epoch in range(epochs):\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","# Evaluate the model on the test set\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data, target in test_loader:\n","        output = model(data)\n","        _, predicted = torch.max(output.data, 1)\n","        total += target.size(0)\n","        correct += (predicted == target).sum().item()\n","\n","print('Accuracy on test set: {:.2f}%'.format(100. * correct / total))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5T4Vrz2wsqU8","outputId":"d534ce30-96ce-449f-c789-dcf400d57476"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.299715\n","Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.274164\n","Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.197592\n","Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.119946\n","Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.099907\n"]}]},{"cell_type":"markdown","source":["\n","\n","**Explanation:**\n","\n","1. **Import Libraries:**\n","   - `torch`: PyTorch for deep learning operations.\n","   - `torch.nn`: PyTorch's neural network module.\n","   - `torch.nn.functional`: PyTorch's functional API for activation functions, etc.\n","   - `torchvision.datasets`: Datasets like MNIST.\n","   - `torchvision.transforms`: For image transformations.\n","   - `torch.utils.data.DataLoader`: For loading and batching data.\n","\n","2. **Attention Module:**\n","   - The `Attention` class remains the same as before.\n","\n","3. **MNIST Classifier Model:**\n","   - `MNISTClassifier` defines a simple convolutional neural network (CNN) for MNIST classification:\n","     - Two convolutional layers with ReLU activation.\n","     - Max pooling to downsample feature maps.\n","     - The `Attention` layer is applied after the convolutional layers to focus on relevant features.\n","     - Two fully connected (FC) layers for classification.\n","\n","4. **Dataset Loading:**\n","   - The MNIST dataset is loaded using `torchvision.datasets.MNIST`.\n","   - `DataLoader` is used to create batches for training and testing.\n","\n","5. **Training:**\n","   - The training loop iterates over epochs and batches.\n","   - The model makes predictions using `model(data)`.\n","   - The loss is calculated using `criterion(output, target)`.\n","   - Backpropagation and optimization are performed using `loss.backward()` and `optimizer.step()`.\n","   - Training progress is printed every 100 batches.\n","\n","6. **Evaluation:**\n","   - The model is evaluated on the test set using `torch.no_grad()` to disable gradient calculations.\n","   - Accuracy is calculated and printed.\n","\n","**Key Points:**\n","\n","- This code demonstrates how to integrate the self-attention mechanism into a CNN model for MNIST classification.\n","- The attention layer helps the model focus on relevant features in the image, potentially improving classification accuracy.\n","- The code provides a basic example. You can experiment with different architectures, hyperparameters, and attention mechanisms to further enhance the model's performance.\n","\n","This code should provide a clear and complete example of how to use the self-attention module with the MNIST dataset.\n"],"metadata":{"id":"MTwWJsrastUk"}},{"cell_type":"code","source":[],"metadata":{"id":"OzZI2t8_sUBU"},"execution_count":null,"outputs":[]}]}